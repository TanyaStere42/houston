# Reference: https://www.elastic.co/guide/en/logstash/5.4/plugins-inputs-jdbc.html


input {
  jdbc {
    jdbc_connection_string => "jdbc:postgresql://${WILDBOOK_DB_HOST}:${WILDBOOK_DB_PORT:5432}/${WILDBOOK_DB_NAME}"
    jdbc_user => "${WILDBOOK_DB_USER}"
    jdbc_password => "${WILDBOOK_DB_PASSWORD}"
    # The driver library is managed by 'logstash-integration-jdbc'
    # But the Postgres JDBC Driver must be installed within '/usr/share/logstash/logstash-core/lib/jars/'
    jdbc_driver_class => "org.postgresql.Driver"
    # runs every 2 minutes
    schedule => "*/2 * * * *"
    statement => '
SELECT
  -- id = UUIDField(required=True)
  mi."ID" as id,
  -- name = Keyword()
  multv."VALUES"::json->\'*\'->>0 as name,
  mi."NICKNAME" as nickname,
  -- alias = Keyword()
      multv."VALUES"::json->\'Alternate ID\'->>0 as alias,
  -- taxonomy = Keyword()
  tax."SCIENTIFICNAME" as taxonomy,
  -- last_sighting = Date()
  -- ... dynamically produced by elasticsearch via inspection of eoncounters
  -- sex = EnumField(Sex, required=False)
  lower(mi."SEX") as sex,
  -- birth = Date(required=False)
  mi."TIMEOFBIRTH" as birth,
  -- death = Date(required=False)
  mi."TIMEOFDEATH" as death,
  -- encounters = []
  -- casting json to text because jdbc is stone age
  array_to_json(array(select row_to_json(enc_row) from(
    SELECT
      -- id = UUIDField(required=True)
      e."ID" as id,
      -- point = GeoPoint(required=True)
      (e."DECIMALLATITUDE"::float || \',\' || e."DECIMALLONGITUDE")::text as point,
      -- json_build_object(\'latitude\', e."DECIMALLATITUDE"::float, \'longitude\', e."DECIMALLONGITUDE"::float) as point,
      -- array_cat(array[e."DECIMALLATITUDE"], array[e."DECIMALLONGITUDE"]) as point,
      -- animate_status = Keyword()
      -- sex = EnumField(Sex, required=False)
      lower(e."SEX") as sex,
      -- submitter_id = Keyword(required=True)
      -- ... unclear where this is going to be coming from; ownership is now in houston
      -- date_occurred = Date()
      (cdt."DATETIME"::timestamp || \' \' || cdt."TIMEZONE")::timestamp with time zone as date_occurred,
      -- taxonomy = Keyword()
      tax."SCIENTIFICNAME" as taxonomy,
      -- has_annotation = Boolean(required=True)
      -- ... this point of data will be in houston
    FROM
      "MARKEDINDIVIDUAL_ENCOUNTERS" as mie
      left join "ENCOUNTER" as e on (mie."ID_EID" = e."ID")
      left join "TAXONOMY" as tax on (e."TAXONOMY_ID_OID" = tax."ID")
      left join "COMPLEXDATETIME" as cdt on (e."TIME_COMPLEXDATETIME_ID_OID" = cdt."COMPLEXDATETIME_ID")
    WHERE mie."ID_OID" = mi."ID"
  ) as enc_row))::text as encounters
FROM
  "MARKEDINDIVIDUAL" as mi
  -- join for name
  left join "MULTIVALUE" as multv on (mi."NAMES_ID_OID" = multv."ID")
  -- join for genus and species
  left join "TAXONOMY" as tax on (mi."TAXONOMY_ID_OID" = tax."ID")
;'
  }
}

filter {
  json {
    source => 'encounters'
    target => 'encounters'
  }
  # ruby {
  #   # simplistic approach to removing null fields
  #   # ref: https://stackoverflow.com/a/28959437/176882
  #   code => "
  #     hash = event.to_hash
  #     hash.each do |k,v|
  #       if v == nil
  #         event.remove(k)
  #       end
  #     end
  #     "
  # }
}

output {
  elasticsearch {
    hosts => [ "http://${ELASTICSEARCH_HOST}:9200" ]
    index => "individuals"
    # Note, the column names get properly lowercased, hence `id` rather than `ID`
    document_id => "markedindividual_%{id}"
    doc_as_upsert => true
  }
}
